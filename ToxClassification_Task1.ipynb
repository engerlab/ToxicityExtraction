{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd15020-be08-4807-bf39-4443ed9dbab2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-25T14:04:50.623920Z",
     "start_time": "2024-12-25T14:04:49.582483Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from utils import Sample, chunk_spans, evaluate\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from transformers import AutoModel, AutoTokenizer, AutoConfig\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def get_files(directory,ext):\n",
    "    doc_files = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith(ext):\n",
    "                doc_files.append(os.path.join(root, file))\n",
    "    return doc_files\n",
    "\n",
    "all_txt=get_files('SampleData','.txt')\n",
    "\n",
    "\n",
    "idx_to_tag = {'O': 0, 'B-Tox': 1, 'I-Tox': 2}\n",
    "\n",
    "samples = list()\n",
    "\n",
    "\n",
    "lm_version='bert-base-cased'\n",
    "\n",
    "tokenizer=AutoTokenizer.from_pretrained(lm_version)\n",
    "\n",
    "for file in all_txt:\n",
    "    text = open(file, 'r').read()\n",
    "\n",
    "    tokenizer_out = tokenizer(text, return_offsets_mapping=True, add_special_tokens=False)\n",
    "\n",
    "    new_sample = Sample(os.path.basename(file), text, tokenizer_out['input_ids'], tokenizer_out['offset_mapping'],\n",
    "                        tokenizer.tokenize(text))\n",
    "\n",
    "    annotation_file = open(file.replace('.txt', '.ann'), 'r')\n",
    "\n",
    "    annotation_concept_map = {}\n",
    "    for line in annotation_file:\n",
    "        line = line.split('\\n')[0].split('\\t')\n",
    "        if line[0][0] == 'T':\n",
    "            type = line[1].split(' ')[0]\n",
    "            id = line[0]\n",
    "            if type == 'SideEffect':\n",
    "                span = (int(line[1].split(' ')[1]), int(line[1].split(' ')[-1]))\n",
    "                new_sample.add_anno(id, 'SideEffect', span)\n",
    "        else:\n",
    "            att_id = line[0]\n",
    "            span_id = line[1].split(' ')[1]\n",
    "            cui = line[1].split(' ')[2]\n",
    "            new_sample.spans[span_id][2] = cui\n",
    "\n",
    "    new_sample.add_labels()\n",
    "\n",
    "    samples.append(new_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57445934-5861-48c9-8ff2-3547ad813e96",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-25T14:04:56.124204Z",
     "start_time": "2024-12-25T14:04:56.118933Z"
    }
   },
   "outputs": [],
   "source": [
    "class Classification(torch.nn.Module):\n",
    "    def __init__(self,language_model):\n",
    "        super(Classification, self).__init__()\n",
    "        self.config = AutoConfig.from_pretrained(language_model)\n",
    "        self.lm=AutoModel.from_pretrained(language_model)\n",
    "        self.num_classes = 2\n",
    "        self.projection=nn.Linear(self.config.hidden_size,self.num_classes)\n",
    "\n",
    "    def forward(self,input_ids):\n",
    "        hiddens=self.lm(input_ids)\n",
    "        return self.projection(hiddens['pooler_output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916f2e1ca465c34b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-25T14:05:26.043609Z",
     "start_time": "2024-12-25T14:04:59.391594Z"
    }
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model=Classification(lm_version).to(device)\n",
    "optimizer=Adam(model.parameters(),lr=0.00001)\n",
    "loss_function=nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "n_epoch=20\n",
    "\n",
    "for epoch in range(n_epoch):\n",
    "    all_loss=list()\n",
    "    for sample in samples:\n",
    "        model.zero_grad()\n",
    "        max_len=model.config.max_position_embeddings\n",
    "\n",
    "        token_ids=torch.tensor([sample.token_ids[0:max_len]],dtype=torch.long).to(device)\n",
    "        pred=model(token_ids)\n",
    "        \n",
    "        label=0\n",
    "        if 1 in sample.labels:\n",
    "            label=1\n",
    "            \n",
    "        target=torch.tensor([label],dtype=torch.long).to(device)\n",
    "        loss=loss_function(pred,target)\n",
    "        all_loss.append(loss.detach().cpu().item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print('Average loss=',np.mean(all_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29896374cc93a162",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-25T14:05:30.719545Z",
     "start_time": "2024-12-25T14:05:30.336418Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    all_gold=list()\n",
    "    all_pred=list()\n",
    "    for sample in samples:\n",
    "        max_len = model.config.max_position_embeddings\n",
    "        token_ids = torch.tensor([sample.token_ids[0:max_len]], dtype=torch.long).to(device)\n",
    "        pred = model(token_ids).cpu().argmax(dim=-1).item()\n",
    "        all_pred.append(pred)\n",
    "        \n",
    "        gold=0\n",
    "        if 1 in sample.labels:\n",
    "            gold=1\n",
    "        \n",
    "        all_gold.append(gold)\n",
    "    \n",
    "    print(classification_report(all_gold, all_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af22159d-26b7-461a-b3d2-c595e8c2d354",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
