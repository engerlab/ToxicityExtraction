{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bfd15020-be08-4807-bf39-4443ed9dbab2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T16:43:12.469008Z",
     "start_time": "2024-12-08T16:43:10.965774Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from utils import Sample, chunk_spans, evaluate\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from transformers import AutoModel, AutoTokenizer, AutoConfig\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def get_files(directory,ext):\n",
    "    doc_files = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith(ext):\n",
    "                doc_files.append(os.path.join(root, file))\n",
    "    return doc_files\n",
    "\n",
    "all_txt=get_files('SampleData','.txt')\n",
    "\n",
    "\n",
    "idx_to_tag = {'O': 0, 'B-Tox': 1, 'I-Tox': 2}\n",
    "\n",
    "samples = list()\n",
    "\n",
    "\n",
    "lm_version='bert-base-cased'\n",
    "\n",
    "tokenizer=AutoTokenizer.from_pretrained(lm_version)\n",
    "\n",
    "for file in all_txt:\n",
    "    text = open(file, 'r').read()\n",
    "\n",
    "    tokenizer_out = tokenizer(text, return_offsets_mapping=True, add_special_tokens=False)\n",
    "\n",
    "    new_sample = Sample(os.path.basename(file), text, tokenizer_out['input_ids'], tokenizer_out['offset_mapping'],\n",
    "                        tokenizer.tokenize(text))\n",
    "\n",
    "    annotation_file = open(file.replace('.txt', '.ann'), 'r')\n",
    "\n",
    "    annotation_concept_map = {}\n",
    "    for line in annotation_file:\n",
    "        line = line.split('\\n')[0].split('\\t')\n",
    "        if line[0][0] == 'T':\n",
    "            type = line[1].split(' ')[0]\n",
    "            id = line[0]\n",
    "            if type == 'SideEffect':\n",
    "                span = (int(line[1].split(' ')[1]), int(line[1].split(' ')[-1]))\n",
    "                new_sample.add_anno(id, 'SideEffect', span)\n",
    "        else:\n",
    "            att_id = line[0]\n",
    "            span_id = line[1].split(' ')[1]\n",
    "            cui = line[1].split(' ')[2]\n",
    "            new_sample.spans[span_id][2] = cui\n",
    "\n",
    "    new_sample.add_labels()\n",
    "\n",
    "    samples.append(new_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "57445934-5861-48c9-8ff2-3547ad813e96",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T16:43:16.655995Z",
     "start_time": "2024-12-08T16:43:16.652202Z"
    }
   },
   "outputs": [],
   "source": [
    "class NER(torch.nn.Module):\n",
    "    def __init__(self,language_model):\n",
    "        super(NER, self).__init__()\n",
    "        self.config = AutoConfig.from_pretrained(language_model)\n",
    "        self.lm=AutoModel.from_pretrained(language_model)\n",
    "        self.num_classes = 3\n",
    "        self.projection=nn.Linear(self.config.hidden_size,self.num_classes)\n",
    "\n",
    "    def forward(self,input_ids):\n",
    "        hiddens=self.lm(input_ids)\n",
    "        return self.projection(hiddens['last_hidden_state']).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "916f2e1ca465c34b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T16:43:44.240778Z",
     "start_time": "2024-12-08T16:43:24.871736Z"
    }
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: unspecified launch failure\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 3\u001b[0m model\u001b[38;5;241m=\u001b[39mNER(lm_version)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      4\u001b[0m optimizer\u001b[38;5;241m=\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(),lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.00001\u001b[39m)\n\u001b[1;32m      5\u001b[0m loss_function\u001b[38;5;241m=\u001b[39mnn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1340\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1337\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1338\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1340\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply(convert)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 900\u001b[0m         module\u001b[38;5;241m.\u001b[39m_apply(fn)\n\u001b[1;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 900\u001b[0m         module\u001b[38;5;241m.\u001b[39m_apply(fn)\n\u001b[1;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 900\u001b[0m         module\u001b[38;5;241m.\u001b[39m_apply(fn)\n\u001b[1;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:927\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    923\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    924\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    925\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    926\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 927\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m fn(param)\n\u001b[1;32m    928\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    930\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1326\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1320\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1321\u001b[0m             device,\n\u001b[1;32m   1322\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1323\u001b[0m             non_blocking,\n\u001b[1;32m   1324\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1325\u001b[0m         )\n\u001b[0;32m-> 1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1327\u001b[0m         device,\n\u001b[1;32m   1328\u001b[0m         dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1329\u001b[0m         non_blocking,\n\u001b[1;32m   1330\u001b[0m     )\n\u001b[1;32m   1331\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: unspecified launch failure\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model=NER(lm_version).to(device)\n",
    "optimizer=Adam(model.parameters(),lr=0.00001)\n",
    "loss_function=nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "n_epoch=20\n",
    "\n",
    "for epoch in range(n_epoch):\n",
    "    all_loss=list()\n",
    "    for sample in samples:\n",
    "        model.zero_grad()\n",
    "        max_len=model.config.max_position_embeddings\n",
    "\n",
    "        token_ids=torch.tensor([sample.token_ids[0:max_len]],dtype=torch.long).to(device)\n",
    "        pred=model(token_ids)\n",
    "        target=torch.tensor(sample.labels[0:max_len],dtype=torch.long).to(device)\n",
    "        loss=loss_function(pred,target)\n",
    "        all_loss.append(loss.detach().cpu().item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print('Average loss=',np.mean(all_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29896374cc93a162",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T03:08:52.330794Z",
     "start_time": "2024-12-08T03:08:52.302027Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'strict': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0}, 'overlapping': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0}}\n",
      "{'strict': {'precision': 1.0, 'recall': 0.3333333333333333, 'f1': 0.5}, 'overlapping': {'precision': 1.0, 'recall': 0.3333333333333333, 'f1': 0.5}}\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    for sample in samples:\n",
    "        max_len = model.config.max_position_embeddings\n",
    "        token_ids = torch.tensor([sample.token_ids[0:max_len]], dtype=torch.long).to(device)\n",
    "        pred = model(token_ids)\n",
    "\n",
    "        pred = list(torch.argmax(pred, dim=1).detach().cpu().numpy())\n",
    "\n",
    "        diff = max(len(sample.token_ids) - max_len, 0)\n",
    "        pred = pred + [0] * diff\n",
    "\n",
    "        pred = [(sample.token_spans[span[0]][0], sample.token_spans[span[1]][1], 'SideEffect') for span in\n",
    "                chunk_spans(pred)]\n",
    "        gold = [(sample.spans[key][1][0], sample.spans[key][1][1], sample.spans[key][0]) for key in\n",
    "                [key for key in sample.spans]]\n",
    "\n",
    "        if len(pred) == 0 and len(gold) == 0:\n",
    "            continue\n",
    "\n",
    "        result=evaluate(gold,pred)\n",
    "        print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af22159d-26b7-461a-b3d2-c595e8c2d354",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
